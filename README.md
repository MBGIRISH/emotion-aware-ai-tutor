# Emotion-Aware AI Tutor

A production-ready multimodal AI tutoring system that adapts to student emotions in real-time using face and voice emotion detection, engagement tracking, and LLM-powered adaptive responses.

## ðŸŽ¯ System Overview

This system combines:
- **Real-time face emotion detection** using FER-2013 trained models
- **Real-time voice emotion detection** using RAVDESS trained models
- **Engagement & confusion tracking** via MediaPipe (blink rate, gaze direction, head pose)
- **Adaptive LLM tutoring** that responds to student emotional state
- **Streamlit dashboard** for real-time visualization and interaction

## ðŸ“ Project Structure

```
emotion-aware-ai-tutor/
 â”œâ”€â”€ README.md
 â”œâ”€â”€ requirements.txt
 â”œâ”€â”€ data/
 â”‚    â”œâ”€â”€ fer2013/               # FER-2013 dataset (upload here)
 â”‚    â”œâ”€â”€ ravdess/               # RAVDESS dataset (upload here)
 â”‚    â”œâ”€â”€ processed/             # Preprocessed data cache
 â”‚    â””â”€â”€ instructions.md        # Dataset upload instructions
 â”œâ”€â”€ models/
 â”‚    â”œâ”€â”€ face_emotion_model.pth
 â”‚    â”œâ”€â”€ audio_emotion_model.pth
 â”‚    â””â”€â”€ README.md
 â”œâ”€â”€ backend/
 â”‚    â”œâ”€â”€ api.py                 # FastAPI server
 â”‚    â”œâ”€â”€ inference_face.py      # Real-time face inference
 â”‚    â”œâ”€â”€ inference_audio.py     # Real-time audio inference
 â”‚    â”œâ”€â”€ engagement.py          # Engagement & confusion logic
 â”‚    â”œâ”€â”€ tutor.py               # LLM adaptation engine
 â”‚    â””â”€â”€ utils/
 â”‚          â”œâ”€â”€ face_detector.py
 â”‚          â”œâ”€â”€ audio_utils.py
 â”‚          â””â”€â”€ logger.py
 â”œâ”€â”€ app/
 â”‚    â”œâ”€â”€ streamlit_app.py       # Main dashboard
 â”‚    â””â”€â”€ components/
 â”‚          â”œâ”€â”€ emotion_meter.py
 â”‚          â”œâ”€â”€ voice_gauge.py
 â”‚          â”œâ”€â”€ engagement_bar.py
 â”‚          â””â”€â”€ tutor_chatbox.py
 â”œâ”€â”€ notebooks/
 â”‚    â”œâ”€â”€ train_face_emotion.ipynb
 â”‚    â”œâ”€â”€ train_audio_emotion.ipynb
 â”‚    â”œâ”€â”€ evaluate_models.ipynb
 â”‚    â””â”€â”€ demo_predictions.ipynb
 â””â”€â”€ utils/
      â”œâ”€â”€ preprocessing_face.py
      â”œâ”€â”€ preprocessing_audio.py
      â””â”€â”€ common.py
```

## ðŸš€ Quick Start

### 1. Installation

```bash
# Create virtual environment
python -m venv venv
source venv/bin/activate  # On Windows: venv\Scripts\activate

# Install dependencies
pip install -r requirements.txt
```

### 2. Dataset Setup

**FER-2013 Dataset:**
- Download FER-2013 from: https://www.kaggle.com/datasets/msambare/fer2013
- Place files in `data/fer2013/`:
  - Option A: `train.csv` and `test.csv` files
  - Option B: Folder structure with `train/` and `test/` subdirectories
- See `data/instructions.md` for detailed instructions

**RAVDESS Dataset:**
- Download RAVDESS from: https://zenodo.org/record/1188976
- Extract the complete dataset to `data/ravdess/`
- Should contain audio files organized by actor, emotion, and statement
- See `data/instructions.md` for detailed instructions

### 3. Data Preprocessing

```bash
# Preprocess FER-2013 data
python utils/preprocessing_face.py

# Preprocess RAVDESS data
python utils/preprocessing_audio.py
```

### 4. Model Training

**Train Face Emotion Model:**
```bash
# Open Jupyter notebook
jupyter notebook notebooks/train_face_emotion.ipynb
# Run all cells to train and save model to models/face_emotion_model.pth
```

**Train Audio Emotion Model:**
```bash
jupyter notebook notebooks/train_audio_emotion.ipynb
# Run all cells to train and save model to models/audio_emotion_model.pth
```

**Evaluate Models:**
```bash
jupyter notebook notebooks/evaluate_models.ipynb
```

### 5. Launch the System

**Start FastAPI Backend:**
```bash
cd backend
uvicorn api:app --reload --host 0.0.0.0 --port 8000
```

**Launch Streamlit Dashboard:**
```bash
streamlit run app/streamlit_app.py
```

The dashboard will open in your browser at `http://localhost:8501`

## ðŸ§  How It Works

### Face Emotion Detection
- Uses MediaPipe for face detection and landmark extraction
- Runs trained CNN/MobileNetV2 model on detected faces
- Outputs 7 emotion probabilities: Happy, Sad, Neutral, Angry, Fear, Disgust, Surprise

### Voice Emotion Detection
- Captures microphone input in real-time
- Extracts MFCC (Mel-frequency cepstral coefficients) features
- Runs trained LSTM/CNN audio classifier
- Outputs emotion probabilities from RAVDESS classes

### Engagement Score Calculation

The engagement score (0-100) combines multiple signals:

1. **Blink Rate** (via Eye Aspect Ratio from MediaPipe)
   - Normal: 15-20 blinks/min
   - High (>30/min): Possible confusion or fatigue
   - Low (<10/min): Possible disengagement

2. **Gaze Direction** (head pose estimation)
   - Looking at screen: +engagement
   - Looking away: -engagement
   - Pitch/yaw/roll angles indicate attention

3. **Emotion Trends**
   - Positive emotions (happy, surprise): +engagement
   - Negative emotions (sad, angry, fear): -engagement
   - Neutral: baseline

4. **Confusion Detection**
   - High blink rate + negative emotions + gaze away = confusion
   - Triggers adaptive tutoring responses

**Formula:**
```
engagement = (
    0.3 * (1 - normalized_blink_deviation) +
    0.3 * gaze_attention_score +
    0.4 * emotion_positive_score
) * 100
```

### LLM Tutor Adaptation

The `tutor.py` module uses emotion and engagement data to:

1. **Detect Confusion**: High confusion triggers simplified explanations
2. **Motivate**: Low engagement triggers encouraging messages
3. **Adapt Pace**: Adjusts explanation complexity based on emotional state
4. **Provide Hints**: Offers contextual hints when student is stuck

**Example Adaptive Responses:**
- Confusion detected â†’ "Let me break this down into simpler steps..."
- Low engagement â†’ "I see you might be finding this challenging. Let's try a different approach!"
- Positive emotions â†’ "Great progress! Let's continue building on this concept."

## ðŸ“Š Dashboard Features

- **Webcam Feed**: Real-time video preview with face detection overlay
- **Emotion Bar Graph**: Live emotion probabilities for 7 face emotions
- **Voice Emotion Meter**: Real-time audio emotion visualization
- **Engagement Score Bar**: 0-100 engagement level with color coding
- **Confusion Alerts**: Pop-up notifications when confusion is detected
- **Tutor Chatbox**: Interactive chat with adaptive LLM tutor
- **Session Analytics**: Historical emotion and engagement trends

## ðŸ”§ Configuration

Create a `.env` file in the project root:

```env
# LLM API Keys (choose one)
OPENAI_API_KEY=your_openai_key_here
ANTHROPIC_API_KEY=your_anthropic_key_here

# Model Paths
FACE_MODEL_PATH=models/face_emotion_model.pth
AUDIO_MODEL_PATH=models/audio_emotion_model.pth

# API Settings
API_HOST=0.0.0.0
API_PORT=8000

# Streamlit Settings
STREAMLIT_PORT=8501
```

## ðŸ§ª Testing

```bash
# Test face inference
python backend/inference_face.py

# Test audio inference
python backend/inference_audio.py

# Test engagement calculation
python -c "from backend.engagement import compute_engagement; print(compute_engagement(...))"
```

## ðŸ“ Development

### Adding New Features

- **New emotion classes**: Modify model architecture in training notebooks
- **New engagement metrics**: Extend `backend/engagement.py`
- **Custom LLM prompts**: Modify `backend/tutor.py`

### Code Style

```bash
# Format code
black backend/ app/ utils/

# Lint code
flake8 backend/ app/ utils/
```

## ðŸ› Troubleshooting

**Webcam not detected:**
- Check camera permissions
- Try different camera index in `inference_face.py`

**Microphone not working:**
- Check system audio permissions
- Install `portaudio` (macOS: `brew install portaudio`)

**Model not found:**
- Ensure models are trained and saved to `models/` directory
- Check model paths in `.env` file

**CUDA/GPU issues:**
- Models default to CPU; modify device in inference scripts for GPU

## ðŸ“„ License

MIT License - See LICENSE file for details

## ðŸ¤ Contributing

Contributions welcome! Please open an issue or submit a pull request.

## ðŸ“§ Contact

For questions or support, please open an issue on GitHub.
